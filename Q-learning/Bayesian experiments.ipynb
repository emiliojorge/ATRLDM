{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://ai.stanford.edu/~nir/Papers/DFR1.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How the uncertainity is modelled ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncertainity is modeled by keeping a distribution over the rewards for each state-action pair that is updated as more evidence appears. In the paper they suggest this is done via a Normal-gamma distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How the actions are selected?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actions are selected such that we maximize the Q(s,a) + VPI(s,a) where VPI is the value of perfect information. VPI is given by the expected \"gain\" for each state-action pair and describes how the $\\max_a Q(s,a)$ is expected to change for the given state. There is a nice closed-form of VPI given in the paper available above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How the uncertainities are updated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import time\n",
    "from lake_envs import *\n",
    "import value_iteration\n",
    "import utility\n",
    "import tabQ_learning\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jorge\\ATRLDM\\Q-learning\\tabQ_learning.py:118: RuntimeWarning: invalid value encountered in true_divide\n",
      "  alpha[state,:]*np.sqrt(2*lam[state,:]))*np.power(\n",
      "C:\\Users\\jorge\\ATRLDM\\Q-learning\\tabQ_learning.py:119: RuntimeWarning: overflow encountered in power\n",
      "  (1+mu0[state,:]**2)/(2*alpha[state,:]), -alpha[state,:]+1/2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function took 666.498 ms\n",
      "Not yet close!! The value of your provided Q is 2.0, and this is not 0.05 close to the optimal value of 0.7737809374999999\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('Deterministic-4x4-FrozenLake-v0')\n",
    "Q = tabQ_learning.bayesian_Qlearning(env, num_observations = 6*10**2)\n",
    "value_iteration.isQvalueErrorEpsilonClose(env, Q, gamma=0.95, epsilon=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
